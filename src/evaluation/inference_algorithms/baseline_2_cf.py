"""
this file is build based on the code found in evaluate_suffix_and_remaining_time.py

here the beam search (with breath-first-search) is implemented, to find compliant prediction

Author: Anton Yeshchenko
"""
from __future__ import division

import csv
import os.path
import pdb
import time
from queue import PriorityQueue
from datetime import timedelta

import distance
import numpy as np
from tensorflow.keras.models import load_model
from sklearn import metrics
from jellyfish._jellyfish import damerau_levenshtein_distance
import shared_variables
from evaluation.server_replayer import verify_formula_as_compliant, verify_formula_ivan
from evaluation.prepare_data import amplify, get_symbol_ampl
from evaluation.prepare_data import encode
from evaluation.prepare_data_resource import prepare_testing_data, select_declare_verified_traces


def run_experiments(log_name, models_folder, fold):
    beam_size = shared_variables.beam_size
    model_filename = shared_variables.extract_last_model_checkpoint(log_name, models_folder, fold, 'CF')
    #declare_model_filename = shared_variables.extract_declare_model_filename(log_name)
    pn_model_filename = shared_variables.extract_petrinet_filename(log_name)

    log_settings_dictionary = shared_variables.log_settings[log_name]
    formula = log_settings_dictionary['formula']
    prefix_size_pred_from = log_settings_dictionary['prefix_size_pred_from']
    prefix_size_pred_to = log_settings_dictionary['prefix_size_pred_to']

    start_time = time.time()

    # prepare the data
    lines, \
    lines_id, \
    lines_group, \
    lines_t, \
    lines_t2, \
    lines_t3, \
    lines_t4, \
    maxlen, \
    chars, \
    chars_group, \
    char_indices, \
    char_indices_group, \
    divisor, \
    divisor2, \
    divisor3, \
    predict_size, \
    target_indices_char, \
    target_indices_char_group, \
    target_char_indices, \
    target_char_indices_group = prepare_testing_data(log_name)

    # this is the beam stack size, means how many "best" alternatives will be stored
    one_ahead_gt = []
    one_ahead_pred = []

    # find cycles and modify the probability functionality goes here
    stop_symbol_probability_amplifier_current = 1

    # load model, set this to the model generated by train.py
    model = load_model(model_filename)

    class NodePrediction:
        def __init__(self, data, crop_line, tot_predicted_time, probability_of=0):
            self.data = data
            self.cropped_line = crop_line
            self.total_predicted_time = tot_predicted_time
            self.probability_of = probability_of

        def __str__(self):
            return f"Prefix: {self.cropped_line}, prob. {self.probability_of}"

        def __lt__(self, other):
            return -self.probability_of < -other.probability_of

    folder_path = shared_variables.outputs_folder + models_folder + '/' + str(fold) + '/results/LTL/'
    if not os.path.exists(folder_path):
        os.makedirs(folder_path)
    output_filename = folder_path + '%s_%s.csv' % (log_name, 'CF')

    with open(output_filename, 'w') as csvfile:
        spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)
        spamwriter.writerow(["Prefix length",
                             "Ground truth",
                             "Predicted",
                             "Damerau-Levenshtein",
                             "Jaccard",
                             "Ground truth times",
                             "Predicted times",
                             "RMSE",
                             "MAE",
                             "Median AE", "compliantness"])

        curr_time = time.time()

        lines_s, \
        lines_id_s, \
        lines_group_s, \
        lines_t_s, \
        lines_t2_s, \
        lines_t3_s, \
        lines_t4_s = select_declare_verified_traces(log_name, lines, lines_id, lines_group, lines_t, lines_t2,
                                                    lines_t3, lines_t4, pn_model_filename, None)

        print("formulas verified: " + str(len(lines_s)) + " out of : " + str(len(lines)))
        print('elapsed_time:', time.time() - curr_time)

        for prefix_size in range(prefix_size_pred_from, prefix_size_pred_to):
            print("prefix size: " + str(prefix_size))
            counter = 0
            for line, times, times2, times3 in zip(lines_s, lines_t_s, lines_t2_s, lines_t3_s):
                times.append(0)
                cropped_line = ''.join(line[:prefix_size])
                cropped_times = times[:prefix_size]
                cropped_times3 = times3[:prefix_size]
                if len(times2) < prefix_size:
                    continue  # make no prediction for this case, since this case has ended already

                # initialize root of the tree for beam search
                total_predicted_time_initialization = 0
                search_node_root = NodePrediction(encode(cropped_line, cropped_times, cropped_times3, maxlen, chars,
                                                         char_indices, divisor, divisor2), cropped_line,
                                                  total_predicted_time_initialization)

                ground_truth = ''.join(line[prefix_size:prefix_size + predict_size])
                ground_truth_t = times2[prefix_size - 1]
                case_end_time = times2[len(times2) - 1]
                ground_truth_t = case_end_time - ground_truth_t

                queue_next_steps = PriorityQueue()
                ##queue_next_steps.put((-search_node_root.probability_of, search_node_root))
                queue_next_steps.put(search_node_root)

                queue_next_steps_future = PriorityQueue()
                start_of_the_cycle_symbol = " "
                found_satisfying_constraint = False

                current_beam_size = beam_size
                current_prediction_premis = None
                counter_id = 0
                #print(f"Prefix {line}")
                for i in range(predict_size):
                    #print(f"Beam search at step {i}")
                    for k in range(current_beam_size):
                        if queue_next_steps.empty():
                            break

                        ##_, current_prediction_premis = queue_next_steps.get()
                        #print(f"k is {k}, queue_next_steps contains the nodes")
                        #for el in queue_next_steps.queue:
                        #    print(f"\t{el}")
                        current_prediction_premis = queue_next_steps.get()

                        #print(f"k is {k}, current node is {current_prediction_premis}")
                        #"""
                        if not found_satisfying_constraint:
                            if verify_formula_ivan(counter_id, current_prediction_premis.cropped_line, log_name, None, "LTL"):
                            #if verify_formula_as_compliant(counter_id, current_prediction_premis.cropped_line, log_name):
                                #print(f"k is {k}, current node {current_prediction_premis} is compliant")
                                # the formula verified and we can just finish the predictions
                                # beam size is 1 because predict only sequence of events
                                current_beam_size = 1
                                current_prediction_premis.probability_of = 0.0
                                # overwrite new queue
                                queue_next_steps_future = PriorityQueue()
                                found_satisfying_constraint = True
                        #"""
                        enc = current_prediction_premis.data
                        temp_cropped_line = current_prediction_premis.cropped_line
                        y = model.predict(enc, verbose=0)  # make predictions
                        # split predictions into separate activity and time predictions
                        y_char = y[0][0]
                        y_t = y[1][0][0]

                        if y_t < 0:
                            y_t = 0
                        cropped_times.append(y_t)

                        if not i == 0:
                            stop_symbol_probability_amplifier_current, start_of_the_cycle_symbol = \
                                amplify(temp_cropped_line)

                        # in not reached, function :choose_next_top_descendant: will backtrack
                        y_t = y_t * divisor3
                        cropped_times3.append(cropped_times3[-1] + timedelta(seconds=(int(y_t) if y_t == 0 else y_t)))
                        #print(f"Adding the next {current_beam_size} nodes")
                        for j in range(current_beam_size):
                            temp_prediction = get_symbol_ampl(y_char, target_indices_char, target_char_indices,
                                                              start_of_the_cycle_symbol,
                                                              stop_symbol_probability_amplifier_current, j)
                            # end of case was just predicted, therefore, stop predicting further into the future
                            if temp_prediction == '!':
                                if verify_formula_ivan(counter_id, temp_cropped_line, log_name, None, "LTL"):
                                #if verify_formula_as_compliant(counter, temp_cropped_line, log_name):
                                    #print(f"{temp_cropped_line} finished and compliant")
                                    one_ahead_pred.append(current_prediction_premis.total_predicted_time)
                                    one_ahead_gt.append(ground_truth_t)
                                    stop_symbol_probability_amplifier_current = 1
                                    # print('! predicted, end case')
                                    queue_next_steps = PriorityQueue()
                                    break
                                else:
                                    #print(f"{temp_cropped_line} finished and NOT compliant")
                                    continue

                            temp_cropped_line = current_prediction_premis.cropped_line + temp_prediction
                            temp_total_predicted_time = current_prediction_premis.total_predicted_time + y_t
                            temp_state_data = encode(temp_cropped_line, cropped_times, cropped_times3, maxlen, chars,
                                                     char_indices, divisor, divisor2)
                            probability_this = np.sort(y_char)[len(y_char) - 1 - j]

                            temp = NodePrediction(temp_state_data, temp_cropped_line, temp_total_predicted_time,
                                                  current_prediction_premis.probability_of + np.log(probability_this))
                            ## queue_next_steps_future.put((-temp.probability_of, temp))
                            queue_next_steps_future.put(temp)

                            # print 'INFORMATION: ' + str(counter) + ' ' + str(i) + ' ' + str(k) + ' ' + str(j) + ' ' + \
                            #       temp_cropped_line[prefix_size:] + "     " + str(temp.probability_of)
                    #print(f"Elements in queue_next_steps_future:")
                    #for el in queue_next_steps_future.queue:
                    #    print(f"\t{el}")
                    queue_next_steps = queue_next_steps_future
                    queue_next_steps_future = PriorityQueue()
                    counter_id += 1

                counter += 1

                if current_prediction_premis is None:
                    print("Cannot find any trace that is compliant with formula given current beam size")
                    break

                output = []

                if current_prediction_premis is None:
                    predicted = u""
                    total_predicted_time = 0
                else:
                    predicted = (current_prediction_premis.cropped_line[prefix_size:])
                    total_predicted_time = current_prediction_premis.total_predicted_time

                compliantness = verify_formula_as_compliant(counter_id, current_prediction_premis.cropped_line,
                                                            log_name)
                compliantness = 1 if compliantness else 0
                #pdb.set_trace()
                if len(ground_truth) > 0:
                    output.append(prefix_size)
                    output.append(ground_truth)
                    output.append(predicted)
                    # output.append(1 - distance.nlevenshtein(predicted, ground_truth))
                    dls = 1 - (damerau_levenshtein_distance(predicted, ground_truth) / max(
                        len(predicted), len(ground_truth)))
                    if dls < 0:
                        dls = 0
                    output.append(dls)
                    output.append(1 - distance.jaccard(predicted, ground_truth))
                    output.append(ground_truth_t)
                    output.append(total_predicted_time)
                    output.append('')
                    output.append(metrics.mean_absolute_error([ground_truth_t], [total_predicted_time]))
                    output.append(metrics.median_absolute_error([ground_truth_t], [total_predicted_time]))
                    output.append(compliantness)
                    spamwriter.writerow(output)

    print("TIME TO FINISH --- %s seconds ---" % (time.time() - start_time))
